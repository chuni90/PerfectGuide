{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 차원 축소(Dimention Reduction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../img/diment_curse.png' width='70%' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../img/feature_select_extraction.png' width='70%' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA 개요 \n",
    "* 고차원의 원본데이터를 저차원의 부분공간으로 투영하는 데이터 축소기법\n",
    "* 예를 들어, 10차원(feature)을 2차원의 부분공간으로 투영하여 축소\n",
    "* PCA는 원본 데이터가 가지는 데이터 변동성을 가장 중요한 정보로 간주하며,   \n",
    "이 변동성에 기반하여 차원 축소를 진행함 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# 사이킷런 내장 데이터 셋 API 호출\n",
    "iris = load_iris()\n",
    "\n",
    "# 넘파이 데이터 셋을 Pandas DataFrame으로 변환\n",
    "columns = ['sepal_length','sepal_width','petal_length','petal_width']\n",
    "irisDF = pd.DataFrame(iris.data , columns=columns)\n",
    "irisDF['target']=iris.target\n",
    "irisDF.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** sepal_length, sepal_width 두개의 속성으로 데이터 산포 시각화 **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#setosa는 세모, versicolor는 네모, virginica는 동그라미로 표현\n",
    "markers=['^', 's', 'o']\n",
    "\n",
    "#setosa의 target 값은 0, versicolor는 1, virginica는 2. 각 target 별로 다른 shape으로 scatter plot \n",
    "for i, marker in enumerate(markers):\n",
    "    x_axis_data = irisDF[irisDF['target']==i]['sepal_length']\n",
    "    y_axis_data = irisDF[irisDF['target']==i]['sepal_width']\n",
    "    plt.scatter(x_axis_data, y_axis_data, marker=marker,label=iris.target_names[i])\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('sepal length')\n",
    "plt.ylabel('sepal width')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 평균이 0, 분산이 1인 정규 분포로 원본 데이터를 변환 **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "iris_scaled = StandardScaler().fit_transform(irisDF.iloc[:, :-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_scaled.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** PCA 변환 수행 **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "#fit( )과 transform( ) 을 호출하여 PCA 변환 데이터 반환\n",
    "pca.fit(iris_scaled)\n",
    "iris_pca = pca.transform(iris_scaled)\n",
    "print(iris_pca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA 변환된 데이터의 컬럼명을 각각 pca_component_1, pca_component_2로 명명\n",
    "pca_columns=['pca_component_1','pca_component_2']\n",
    "irisDF_pca = pd.DataFrame(iris_pca,columns=pca_columns)\n",
    "irisDF_pca['target']=iris.target\n",
    "irisDF_pca.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** PCA로 차원 축소된 피처들로 데이터 산포도 시각화 **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setosa를 세모, versicolor를 네모, virginica를 동그라미로 표시\n",
    "markers=['^', 's', 'o']\n",
    "\n",
    "#pca_component_1 을 x축, pc_component_2를 y축으로 scatter plot 수행. \n",
    "for i, marker in enumerate(markers):\n",
    "    x_axis_data = irisDF_pca[irisDF_pca['target']==i]['pca_component_1']\n",
    "    y_axis_data = irisDF_pca[irisDF_pca['target']==i]['pca_component_2']\n",
    "    plt.scatter(x_axis_data, y_axis_data, marker=marker,label=iris.target_names[i])\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('pca_component_1')\n",
    "plt.ylabel('pca_component_2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 각 PCA Component별 변동성 비율 **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 원본 데이터와 PCA 변환된 데이터 기반에서 예측 성능 비교 **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "rcf = RandomForestClassifier(random_state=156)\n",
    "scores = cross_val_score(rcf, iris.data, iris.target,scoring='accuracy',cv=3)\n",
    "print(scores)\n",
    "print(np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_X = irisDF_pca[['pca_component_1', 'pca_component_2']]\n",
    "scores_pca = cross_val_score(rcf, pca_X, iris.target, scoring='accuracy', cv=3 )\n",
    "print(scores_pca)\n",
    "print(np.mean(scores_pca))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 신용카드 데이터 세트 PCA 변환\n",
    "\n",
    "** 데이터 로드 및 컬럼명 변환 **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel('pca_credit_card.xls', sheet_name='Data', header=1)\n",
    "print(df.shape)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={'PAY_0':'PAY_1','default payment next month':'default'}, inplace=True)\n",
    "y_target = df['default']\n",
    "# ID, default 컬럼 Drop\n",
    "X_features = df.drop(['ID','default'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_features.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 피처간 상관도 시각화 **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "corr = X_features.corr()\n",
    "plt.figure(figsize=(14,14))\n",
    "sns.heatmap(corr, annot=True, fmt='.1g')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**상관도가 높은 피처들의 PCA 변환 후 변동성 확인**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#BILL_AMT1 ~ BILL_AMT6까지 6개의 속성명 생성\n",
    "cols_bill = ['BILL_AMT'+str(i) for i in range(1, 7)]\n",
    "print('대상 속성명:', cols_bill)\n",
    "\n",
    "# 2개의 PCA 속성을 가진 PCA 객체 생성하고, explained_variance_ratio_ 계산을 위해 fit( ) 호출\n",
    "scaler = StandardScaler()\n",
    "df_cols_scaled = scaler.fit_transform(X_features[cols_bill])\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(df_cols_scaled)\n",
    "\n",
    "print('PCA Component별 변동성:', pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 원본 데이터 세트와 6개 컴포넌트로 PCA 변환된 데이터 세트로 분류 예측 성능 비교 **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "rcf = RandomForestClassifier(n_estimators=300, random_state=156)\n",
    "scores = cross_val_score(rcf, X_features, y_target, scoring='accuracy', cv=3 )\n",
    "\n",
    "print('CV=3 인 경우의 개별 Fold세트별 정확도:',scores)\n",
    "print('평균 정확도:{0:.4f}'.format(np.mean(scores)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 원본 데이터셋에 먼저 StandardScaler적용\n",
    "scaler = StandardScaler()\n",
    "df_scaled = scaler.fit_transform(X_features)\n",
    "\n",
    "# 6개의 Component를 가진 PCA 변환을 수행하고 cross_val_score( )로 분류 예측 수행. \n",
    "pca = PCA(n_components=6)\n",
    "df_pca = pca.fit_transform(df_scaled)\n",
    "scores_pca = cross_val_score(rcf, df_pca, y_target, scoring='accuracy', cv=3)\n",
    "\n",
    "print('CV=3 인 경우의 PCA 변환된 개별 Fold세트별 정확도:',scores_pca)\n",
    "print('PCA 변환 데이터 셋 평균 정확도:{0:.4f}'.format(np.mean(scores_pca)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
