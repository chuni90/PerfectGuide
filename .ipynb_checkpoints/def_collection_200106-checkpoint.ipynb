{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 각 칼럼별 unique value 확인\n",
    "* 칼럼별로 어떤 value를 가지고 있는 지\n",
    "* 그리고 각 비중(%) 어떻게 되는 지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count 파라미터는 unique value를 몇 개까지만 볼꺼냐?\n",
    "# 예) Sex는 남, 여 2개\n",
    "# 예2) 연령대는 10대, 20대, ~ , 60대 6개이므로 count가 5일때는 표시되지 않음\n",
    "\n",
    "def col_value_check(df,count) :\n",
    "    for i in df.columns :\n",
    "        print(\"{}'s Nunique: {}\\n\".format(i,df[i].nunique()))\n",
    "\n",
    "        if df[i].nunique() < count :\n",
    "            print(\"{:=^20}\".format(i))\n",
    "            print(round(df[i].value_counts() / df[i].value_counts().sum(),2))\n",
    "            print(\"{:=^20}\".format(\"=\"))\n",
    "    return\n",
    "col_value_check(test,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NaN Table\n",
    "* 적용 상황\n",
    "    - train, test의 각 칼럼별 NaN 값을 확인하고자 할 때\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train 및 test에서 각각의 na 값을 DF형태로 반환\n",
    "\n",
    "def make_na_table(train, test):\n",
    "    train_na = train.isna().sum()\n",
    "    test_na = test.isna().sum()\n",
    "    na_table = pd.concat((train_na, test_na), join='outer', axis=1, sort=False, keys=('train','test'))\n",
    "    return na_table\n",
    "na_table = make_na_table(train,test)\n",
    "na_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* kaggle에서 발췌한 missing value df 만들기 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_missing_data(df):\n",
    "    flag=df.isna().sum().any()\n",
    "    if flag==True:\n",
    "        total = df.isnull().sum()\n",
    "        percent = (df.isnull().sum())/(df.isnull().count()*100)\n",
    "        output = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "        data_type = []\n",
    "        # written by MJ Bahmani\n",
    "        for col in df.columns:\n",
    "            dtype = str(df[col].dtype)\n",
    "            data_type.append(dtype)\n",
    "        output['Types'] = data_type\n",
    "        return(np.transpose(output))\n",
    "    else:\n",
    "        return(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unique value Table\n",
    "* 적용 상황\n",
    "    - train, test의 각 칼럼별 unique value을 확인하고자 할 때\n",
    "    - columns이 index로 들어감\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_unique_value_table(train, test) :\n",
    "    a = []\n",
    "    b = []\n",
    "\n",
    "    for x in range(0, len(train.columns)) :\n",
    "        a.append(train[train.columns[x]].nunique())\n",
    "        b.append(train.columns[x])\n",
    "    a1 = pd.Series(a, index=b)\n",
    "\n",
    "    c = []\n",
    "    d = []\n",
    "    for x in range(0, len(test.columns)) :\n",
    "        c.append(int(test[test.columns[x]].nunique()))\n",
    "        d.append(test.columns[x])\n",
    "\n",
    "    a2 = pd.Series(c, index=d)\n",
    "\n",
    "    unique_table = pd.concat([a1,a2], axis=1, sort=False, keys=['train', 'test'])\n",
    "    return unique_table\n",
    "unique_table = make_unique_value_table(train, test)\n",
    "unique_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IQR 로 Outlier 찾아내는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier 찾아내는 함수\n",
    "def get_outlier(df, col, weight=1.5) :\n",
    "    col = df[col]\n",
    "    quan_25 = np.percentile(col.values, 25)\n",
    "    quan_75 = np.percentile(col.values, 75)\n",
    "    \n",
    "    # IQR을 구하고, 1.5를 곱해서 최대/최소를 구함\n",
    "    iqr = quan_75 - quan_25\n",
    "    iqr_weight = iqr * weight\n",
    "    min_val = quan_25 - iqr_weight\n",
    "    max_val = quan_75 + iqr_weight\n",
    "    outlier_index = col[(col < min_val) | (col > max_val)].index\n",
    "    return outlier_index\n",
    "\n",
    "# 사용 예제\n",
    "\n",
    "for i in ride_col :\n",
    "    outlier_index = get_outlier(train2, i)\n",
    "    print(\"{}: {}\".format(i,train2.loc[outlier_index][i].shape[0]))\n",
    "print()\n",
    "for i in off_col :\n",
    "    outlier_index = get_outlier(train2, i)\n",
    "    print(\"{}: {}\".format(i,train2.loc[outlier_index][i].shape[0]))\n",
    "print()\n",
    "\n",
    "outlier_index = get_outlier(train2, '18~20_ride')\n",
    "print(\"{}: {}\".format('18~20_ride',train2.loc[outlier_index]['18~20_ride'].shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 이상치 제거 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이상치 제거 함수\n",
    "\n",
    "def del_outlier(df, col_list) :\n",
    "    df_copy = df.copy()\n",
    "    for i in col_list :\n",
    "        outlier_index = get_outlier(df_copy, i)\n",
    "        df_copy.drop(outlier_index, axis=0, inplace=True)\n",
    "        print(i)\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA 및 Randomforest 산출 함수\n",
    "* 적용 상황\n",
    "    - PCA를 통해 차원 축소가 필요한 데, 몇 개의 component가 필요한 지 모를 때\n",
    "    - components 수에 따른 R2(모델설명력)와 Randomforest의 score를 산출함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 컬럼 최대개수에 맞게 components 수를 변화시키면서 설명력을 확인함\n",
    "# \n",
    "\n",
    "def pca_rcf(train_x, train_y, test_x, test_y) :\n",
    "    \n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.decomposition import PCA\n",
    "    \n",
    "    max_sum, max_com, max_result = 0.0, 0, 0.0\n",
    "    final_rcf = RandomForestClassifier()\n",
    "    \n",
    "    # components를 2~5개까지 테스트 해봄\n",
    "    for i in range(2, 5) :\n",
    "        pca = PCA(n_components=i)\n",
    "        pca_train_x = pca.fit_transform(train_x)\n",
    "        imp = []\n",
    "        \n",
    "        for p in pca.explained_variance_ratio_ :\n",
    "            imp.append(round(p,4))\n",
    "            \n",
    "        print(\"components:\",i,\"\\tTot:\",sum(imp))\n",
    "        rcf = RandomForestClassifier(n_estimators=1000, random_state=2442)\n",
    "        rcf.fit(train_x, train_y)\n",
    "        predicted = rcf.predict(test_x)\n",
    "        result = rcf.score(test_x, test_y)\n",
    "        print(\"\\tScore: {:.3f}\".format(result))\n",
    "        \n",
    "        if result > max_result :\n",
    "            max_result = result\n",
    "            final_rcf = rcf\n",
    "\n",
    "        if sum(imp) > max_sum :\n",
    "            max_sum = sum(imp)\n",
    "            max_com = i\n",
    "            \n",
    "    print(\"최종 >> 컴포넌트수: {}, R2: {:.3f}, 예측 정확도: {:.3f}\".format(max_com, max_sum, max_result))\n",
    "    # return 값으로 rcf 모델과 예측값을 반환함 (필요에 따라 수정혀)\n",
    "    return final_rcf, predicted\n",
    "\n",
    "rcf, predicted = pca_rcf(train_x, train_y, test_x, test_y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 성능 평가\n",
    "\n",
    "* classification 평가\n",
    "    * classification report\n",
    "    * roc_auc\n",
    "    * confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_train_eval(model, train_x,  test_x, train_y, test_y) :\n",
    "    from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
    "    \n",
    "    model.fit(train_x, train_y)\n",
    "    pred = model.predict(test_x)\n",
    "    print(\"==Confusion_matrix below==\\n\",confusion_matrix(test_y, pred))\n",
    "    print(classification_report(test_y, pred, digits=3))  # digit, 3자리 수까지만 표기\n",
    "    print()\n",
    "    print(\"{:>12}\\t{:.3f}\".format(\"ROC-AUC\",roc_auc_score(test_y, pred))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearchCV - 교차 검증과 체적 하이퍼 파라미터 튜닝을 한 번에"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSerchCV\n",
    "\n",
    "params = {\n",
    "    \"\" : [],\n",
    "    \"\" : []\n",
    "}\n",
    "\n",
    "grid_cv = GridSerchCV(model, param_grid = params, cv = 2, n_jobs = -1)\n",
    "grid_cv.fit(train_x, train_y)\n",
    "print(\"Best Params:\\n\", grid_cv.best_params_)\n",
    "print(\"Best predict score: {:.4f}\".format(grid_cv.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 전화번호, URL, PRICE 필터링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phone_number_filter(text):\n",
    "    re_pattern = r'\\d{2,3}[-\\.\\s]*\\d{3,4}[-\\.\\s]*\\d{4}(?!\\d)'\n",
    "    new_text = re.sub(re_pattern, 'tel', text)\n",
    "    re_pattern = r'\\(\\d{3}\\)\\s*\\d{4}[-\\.\\s]??\\d{4}'\n",
    "    new_text = re.sub(re_pattern, 'tel', new_text)\n",
    "    return new_text\n",
    "    \n",
    "    \n",
    "def url_filter(text):\n",
    "    re_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),|]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "    new_text = re.sub(re_pattern, 'url', text)\n",
    "    return new_text\n",
    "\n",
    "\n",
    "def price_filter(text):\n",
    "    re_pattern = r'\\d{1,3}[,\\.]\\d{1,3}[만\\천]?\\s?[원]|\\d{1,5}[만\\천]?\\s?[원]'\n",
    "    text = re.sub(re_pattern, 'money', text)\n",
    "    re_pattern = r'[일/이/삼/사/오/육/칠/팔/구/십/백][만\\천]\\s?[원]'\n",
    "    text = re.sub(re_pattern, 'money', text)\n",
    "    re_pattern = r'(?!-)\\d{2,4}[0]{2,4}(?!년)(?!.)|\\d{1,3}[,/.]\\d{3}'\n",
    "    new_text = re.sub(re_pattern, 'money', text)\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 샘플링\n",
    "* Imbalanced Data Set에 적용\n",
    "* 모든 알고리즘을 적용한 것은 아님\n",
    "* [imblearn site](https://imbalanced-learn.readthedocs.io/en/stable/api.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function make_sampling in module __main__:\n",
      "\n",
      "make_sampling(X_train, y_train)\n",
      "    https://imbalanced-learn.readthedocs.io/en/stable/api.html\n",
      "    현재 5개만 추가된 상태\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def make_sampling(X_train, y_train) :\n",
    "    '''https://imbalanced-learn.readthedocs.io/en/stable/api.html\n",
    "    현재 5개만 추가된 상태'''\n",
    "    \n",
    "    from imblearn.under_sampling import EditedNearestNeighbours, NeighbourhoodCleaningRule, TomekLinks\n",
    "    from imblearn.over_sampling import SVMSMOTE\n",
    "    from imblearn.combine import SMOTEENN\n",
    "    from imblearn.metrics import sensitivity_score, specificity_score\n",
    "    \n",
    "    # Edited Nearest Neighbours (이웃 개수 및 kidn_sel ('all'/ 'mode') 둘 중 한개 택\n",
    "    x_enn, y_enn = EditedNearestNeighbours(sampling_strategy='auto', n_neighbors=3, \n",
    "                                           kind_sel='all', n_jobs=-1).fit_sample(X_train, y_train)\n",
    "    \n",
    "    # threshold_cleaing은 임계값\n",
    "    x_ncr, y_ncr = NeighbourhoodCleaningRule(sampling_strategy='auto', n_neighbors=3,\n",
    "                                             kind_sel='all', threshold_cleaning=0.5, n_jobs=-1).fit_sample(X_train, y_train)\n",
    "    \n",
    "    # SMOTE + ENN : only can use 'binary' \n",
    "    x_s_enn, y_s_enn = SMOTEENN(sampling_strategy='auto', random_state=None, n_jobs=-1).fit_sample(X_train, y_train)\n",
    "    \n",
    "    # Tomek's llnk\n",
    "    x_t, y_t = TomekLinks(n_jobs=-1).fit_sample(X_train, y_train)\n",
    "    \n",
    "    x_s_s, y_s_s = SVMSMOTE(n_jobs=-1).fit_sample(X_train, y_train)\n",
    "    \n",
    "    sampling_list = [x_enn, y_enn, x_ncr, y_ncr, x_s_enn, y_s_enn, x_t, y_t, x_s_s, y_s_s]\n",
    "    \n",
    "    return sampling_list\n",
    "\n",
    "help(make_sampling)\n",
    "\n",
    "###################\n",
    "\n",
    "def get_eval(y_test, pred) :\n",
    "    from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score \n",
    "    \n",
    "    cm = confusion_matrix(y_test, pred)\n",
    "    acc = np.round(accuracy_score(y_test, pred),3)\n",
    "    pre = np.round(precision_score(y_test, pred),3)\n",
    "    recall = np.round(recall_score(y_test, pred),3)\n",
    "    f1 = np.round(f1_score(y_test, pred),3)\n",
    "    roc_auc = np.round(roc_auc_score(y_test, pred),3)\n",
    "\n",
    "    return cm, acc, pre, recall, f1, roc_auc\n",
    "######################\n",
    "\n",
    "def make_series(y_test, pred, index_name) :\n",
    "    cm, acc, pre, recall, f1, roc_auc = get_eval(y_test, pred)\n",
    "    score = pd.Series({'c_m':cm, 'acc': acc, 'precision':pre, 'recall':recall, 'f1':f1, 'roc_auc':roc_auc},\n",
    "                             name=index_name)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위에서 샘플링된 것으로 모델에 넣는 것\n",
    "# 현재는 LGBM을 기본 모델로 채택\n",
    "\n",
    "def use_sampling(sampling_list, x_test, y_test) :\n",
    "    # sampling api 리스트\n",
    "    api = ['ENN', 'NCR', 'SMOTEENN', 'Tomek', 'SVMSMOTE']\n",
    "    \n",
    "    from lightgbm import LGBMClassifier\n",
    "\n",
    "    lgbm = LGBMClassifier(n_estimators=200, n_jobs=-1, random_state=1, learning_rate=0.1)\n",
    "    evals = [(x_test, y_test)]\n",
    "    score_df = pd.DataFrame(columns=['c_m','acc','precision','recall','f1','roc_auc'])\n",
    "    \n",
    "    for i in range(0,len(sampling_list),2) :\n",
    "        lgbm.fit(sampling_list[i], sampling_list[i+1], early_stopping_rounds=50, eval_metric=\"auc\", eval_set=evals, verbose=False)\n",
    "        pred = lgbm.predict(x_test)\n",
    "        \n",
    "        if i != 0 :\n",
    "            score = make_series(y_test, pred, api[int(i/2)])\n",
    "            score_df = score_df.append(score)\n",
    "        else :\n",
    "            score = make_series(y_test, pred, api[i])\n",
    "            score_df = score_df.append(score)\n",
    "    \n",
    "    return score_df\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 실사용 예시 (x_train, y_train, x_test, y_test 모두 있어야함)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_list = make_sampling(x_train, y_train)\n",
    "print(len(sampling_list))  # 모델별로 2개씩(x, y) 들어가므로 총 10개가 반환\n",
    "\n",
    "score_df = use_sampling(sampling_list, x_test, y_test)\n",
    "score_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaled 및 선형 회귀모델에 따른 회귀계수, RMSE 차이 확인\n",
    "* 사용된 회귀 : Linear, Ridge, Lasso, ElasticNet\n",
    "* 평가지표 : RMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Ridge 고정 | 스케일별 & alpha별 RMSE df 산출** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# get_scaled_data 함수\n",
    "# method는 표준 정규 분포 변환(Standard), 최대값/최소값 정규화(MinMax), 로그변환(Log) 결정\n",
    "# p_degree는 다향식 특성을 추가할 때 적용. p_degree는 2이상 부여하지 않음\n",
    "def get_scaled_data(method='None', p_degree=None, input_data=None):\n",
    "    if method == 'Standard':\n",
    "        scaled_data = StandardScaler().fit_transform(input_data)\n",
    "    elif method == 'MinMax':\n",
    "        scaled_data = MinMaxScaler().fit_transform(input_data)\n",
    "    elif method == 'Log':\n",
    "        scaled_data = np.log1p(input_data)\n",
    "    else:\n",
    "        scaled_data = input_data\n",
    "\n",
    "    if p_degree != None:\n",
    "        scaled_data = PolynomialFeatures(degree=p_degree, \n",
    "                                         include_bias=False).fit_transform(scaled_data)\n",
    "    \n",
    "    return scaled_data\n",
    "#############################################\n",
    "\n",
    "# alpha값에 따른 회귀 모델의 폴드 평균 RMSE를 출력하고 RMSE값을 DataFrame으로 반환 \n",
    "def get_linear_reg_eval_1000(rmse_df, model_name, params=None, X_data_n=None, y_target_n=None, method=None):\n",
    "    rmse_list = []\n",
    "    print('{:=^30}'.format(model_name))\n",
    "    for param in params:\n",
    "        if model_name =='Ridge': model = Ridge(alpha=param)\n",
    "        neg_mse_scores = cross_val_score(model, X_data_n, \n",
    "                                             y_target_n, scoring=\"neg_mean_squared_error\", cv = 5)\n",
    "        avg_rmse = np.mean(np.sqrt(-1 * neg_mse_scores))\n",
    "        print('alpha {0}일 때 5 폴드 세트의 평균 RMSE: {1:.3f} '.format(param, avg_rmse))\n",
    "        rmse_list.append(np.round(avg_rmse, 2))\n",
    "    rmse_series = pd.Series(rmse_list, index=params, name=method)\n",
    "    rmse_df[method] = rmse_series\n",
    "    return rmse_df\n",
    "##############################################\n",
    "\n",
    "# Ridge의 alpha값을 다르게 적용하고 다양한 데이터 변환방법에 따른 RMSE 추출. \n",
    "alphas = [0.1, 1, 10, 100]\n",
    "#변환 방법은 모두 6개, 원본 그대로, 표준정규분포, 표준정규분포+다항식 특성\n",
    "# 최대/최소 정규화, 최대/최소 정규화+다항식 특성, 로그변환 \n",
    "scale_methods=[(None, None), ('Standard', None), ('Standard_Poly', 2), \n",
    "               ('MinMax', None), ('MinMax_Poly', 2), ('Log', None)]\n",
    "rmse_df = pd.DataFrame()\n",
    "for scale_method in scale_methods:\n",
    "    X_data_scaled = get_scaled_data(method=scale_method[0], p_degree=scale_method[1], \n",
    "                                    input_data=X_data)\n",
    "    print('\\n## 변환 유형:{0}, Polynomial Degree:{1}'.format(scale_method[0], scale_method[1]))\n",
    "    rmse_df = get_linear_reg_eval_1000(rmse_df, 'Ridge', params=alphas, X_data_n=X_data_scaled, y_target_n=y_target,\n",
    "                             method=scale_method[0], verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 표로 확인\n",
    "rmse_df = rmse_df.transpose().reset_index()\n",
    "rmse_df.rename({'index':'method'},inplace=True, axis=1)\n",
    "rmse_df.iloc[0,0] = 'Default'\n",
    "rmse_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot 확인\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(rmse_df.iloc[:,1:])\n",
    "plt.xticks(ticks=[0,1,2,3,4,5], labels=rmse_df['method'])\n",
    "plt.legend(rmse_df.columns[1:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. 선형모델에 따른 평가 및 회귀계수 그래프**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_linear_eval_coeff(model_name, params=None, x_train=None, y_train=None, graph=True):\n",
    "    coeff_df = pd.DataFrame()\n",
    "    coeff_list = []\n",
    "    print('{:=^30}'.format(model_name))\n",
    "    if graph == True : \n",
    "        # alpha가 리스트가 최소 5개\n",
    "        fig , axs = plt.subplots(figsize=(20, 5) , nrows=1, ncols=5)\n",
    "        for pos , param in enumerate(params) :\n",
    "            if model_name =='Ridge': model = Ridge(alpha=param)\n",
    "            elif model_name =='Lasso': model = Lasso(alpha=param)\n",
    "            elif model_name =='ElasticNet': model = ElasticNet(alpha=param, l1_ratio=0.7)\n",
    "            neg_mse_scores = cross_val_score(model, x_train, \n",
    "                                                 y_train, scoring=\"neg_mean_squared_error\", cv = 5)\n",
    "            avg_rmse = np.mean(np.sqrt(-1 * neg_mse_scores))\n",
    "            print('alpha {0}일 때 5 폴드 세트의 평균 RMSE: {1:.4f} '.format(param, avg_rmse))\n",
    "            model.fit(x_train , y_train)\n",
    "            coeff = pd.Series(data=model.coef_ , index=x_train.columns )\n",
    "            coeff = coeff.sort_values(ascending=False)[:20]\n",
    "            coeff_list.append([coeff.values, coeff.index])\n",
    "            colname='alpha:'+str(param)\n",
    "            coeff_df[colname] = coeff\n",
    "            axs[pos].set_title(colname)\n",
    "            axs[pos].set_xlim(-2, 2)\n",
    "            sns.barplot(x=coeff.values , y=coeff.index, ax=axs[pos])\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        return coeff_df, coeff_list, fig\n",
    "    else :\n",
    "        for pos , param in enumerate(params) :\n",
    "            if model_name =='Ridge': model = Ridge(alpha=param)\n",
    "            elif model_name =='Lasso': model = Lasso(alpha=param)\n",
    "            elif model_name =='ElasticNet': model = ElasticNet(alpha=param, l1_ratio=0.7)\n",
    "            neg_mse_scores = cross_val_score(model, x_train, \n",
    "                                                 y_train, scoring=\"neg_mean_squared_error\", cv = 5)\n",
    "            avg_rmse = np.mean(np.sqrt(-1 * neg_mse_scores))\n",
    "            print('alpha {0}일 때 5 폴드 세트의 평균 RMSE: {1:.4f} '.format(param, avg_rmse))\n",
    "            model.fit(x_train , y_train)\n",
    "            coeff = pd.Series(data=model.coef_ , index=x_train.columns )\n",
    "            coeff = coeff.sort_values(ascending=False)\n",
    "            colname='alpha:'+str(param)\n",
    "            coeff_df[colname] = coeff\n",
    "        return coeff_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, coeff_list, fig1 = get_linear_eval_coeff('Ridge', ridge_alphas, X_data, y_target, True)\n",
    "df, coeff_list, fig2 = get_linear_eval_coeff('Lasso', lasso_alphas, X_data, y_target, True)\n",
    "df, coeff_list, fig3 = get_linear_eval_coeff('ElasticNet', elastic_alphas, X_data, y_target, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 모델별 그래프만 보려면 각 셀에서 독립적으로 fig 실행\n",
    "fig1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 차원 축소\n",
    "* PCA, LDA(Linear Discriminant Analysis), Truncked SVD, NMF(Non-negative Matrix Factorization)   \n",
    "* 기본 scaler는 책에서 추천한 대로 Standard Scaler를 사용함   \n",
    "    * 그랬더니 TSVD와 PCA가 거의 동일한 형태임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dimension_reduction(feature, components, target=None) :\n",
    "    \n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.decomposition import PCA, TruncatedSVD, NMF\n",
    "    from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "    \n",
    "    def get_pca(feature, components) :\n",
    "        pca = PCA(n_components=components)\n",
    "        pca.fit(feature)\n",
    "        feature_pca = pca.transform(feature)\n",
    "        print(\"PCA : {}\\nSum: {:.3f}\".format(pca.explained_variance_ratio_,np.sum(pca.explained_variance_ratio_)))\n",
    "        return feature_pca\n",
    "\n",
    "    def get_tsvd(feature, components) :\n",
    "        tsvd = TruncatedSVD(n_components=components)\n",
    "        tsvd.fit(feature)\n",
    "        feature_tsvd = tsvd.transform(feature)\n",
    "        print(\"TSVD : {}\\nSum: {:.3f}\".format(tsvd.explained_variance_ratio_,np.sum(tsvd.explained_variance_ratio_)))\n",
    "        return feature_tsvd\n",
    "\n",
    "    def get_lda(feature, components, target) :\n",
    "        lda = LinearDiscriminantAnalysis(n_components=components)\n",
    "        lda.fit(feature, target)\n",
    "        feature_lda = lda.transform(feature)\n",
    "        print(\"LDA : {}\\nSum: {:.3f}\".format(lda.explained_variance_ratio_, np.sum(lda.explained_variance_ratio_)))\n",
    "        return feature_lda\n",
    "\n",
    "    def get_nmf(feature, components) :\n",
    "        nmf = NMF(n_components=components)\n",
    "        nmf.fit(feature)\n",
    "        feature_nmf = nmf.transform(feature)\n",
    "        return feature_nmf\n",
    "    \n",
    "    # StandardScaler 적용\n",
    "#     feature_scaled = StandardScaler().fit_transform(feature)\n",
    "    \n",
    "    # Scaler 적용 안하려면 \n",
    "    feature_scaled = feature\n",
    "    \n",
    "    if len(target) > 0 and np.min(feature_scaled).min() >= 0 :\n",
    "        \n",
    "        feature_pca = get_pca(feature_scaled, components)\n",
    "        feature_tsvd = get_tsvd(feature_scaled, components)\n",
    "        feature_lda = get_lda(feature_scaled, components, target)\n",
    "        feature_nmf = get_nmf(feature_scaled, components)\n",
    "\n",
    "        return feature_pca, feature_tsvd, feature_nmf, feature_lda\n",
    "    \n",
    "    elif len(target) < 0 and np.min(feature_scaled).min() >= 0 :\n",
    "        \n",
    "        feature_pca = get_pca(feature_scaled, components)\n",
    "        feature_tsvd = get_tsvd(feature_scaled, components)\n",
    "        feature_nmf = get_nmf(feature_scaled, components)\n",
    "        \n",
    "        return feature_pca, feature_tsvd, feature_nmf\n",
    "    \n",
    "    elif len(target) < 0 and np.min(feature_scaled).min() < 0 :\n",
    "        feature_pca = get_pca(feature_scaled, components)\n",
    "        feature_tsvd = get_tsvd(feature_scaled, components)\n",
    "        \n",
    "        return feature_pca, feature_tsvd\n",
    "    \n",
    "    elif len(target) > 0 and np.min(feature_scaled).min() < 0 :\n",
    "        feature_pca = get_pca(feature_scaled, components)\n",
    "        feature_tsvd = get_tsvd(feature_scaled, components)\n",
    "        feature_lda = get_lda(feature_scaled, components, target)\n",
    "        \n",
    "        return feature_pca, feature_tsvd, feature_lda "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예시\n",
    "\n",
    "x = iris.data\n",
    "y = iris.target\n",
    "\n",
    "feature_pca, feature_tsvd, feature_nmf, feature_lda = get_dimension_reduction(x, components=2 ,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 차원축소된 arrayy를 DataFrame으로 변환\n",
    "\n",
    "def create_df(feature_, columns, target) :\n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame(feature_, columns=columns)\n",
    "    df['target'] = target\n",
    "    return df\n",
    "\n",
    "col = ['1_com', '2_com']\n",
    "pca_df = create_df(feature_pca, col, y)\n",
    "tsvd_df = create_df(feature_tsvd, col, y)\n",
    "lda_df = create_df(feature_lda, col, y)\n",
    "nmf_df = create_df(feature_nmf, col, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 차원이 2개일 때만 그릴 수 있음\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "f, ax = plt.subplots(2,2, figsize=(12,8))\n",
    "\n",
    "ax[0,0].set_title('PCA')\n",
    "sns.scatterplot(x='1_com', y='2_com', data=pca_df, hue='target', ax=ax[0,0])\n",
    "\n",
    "ax[0,1].set_title('TSVD')\n",
    "sns.scatterplot(x='1_com', y='2_com', data=tsvd_df, hue='target', ax=ax[0,1])\n",
    "\n",
    "ax[1,0].set_title('LDA')\n",
    "sns.scatterplot(x='1_com', y='2_com', data=lda_df, hue='target', ax=ax[1,0])\n",
    "\n",
    "ax[1,1].set_title('NMF')\n",
    "sns.scatterplot(x='1_com', y='2_com', data=nmf_df, hue='target', ax=ax[1,1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 군집화(Clustering)에 따른 실루엣 스코어 확인\n",
    "\n",
    "* K-means, GMM 2개의 군집 모델에 적용 가능한 실루엣 plot 을 그려주는 코드\n",
    "* 한계\n",
    "1. 실루엣 스코어 만으로 클러스터링의 정확도를 판단하는 것은 무리\n",
    "2. 실제 분포(시각화)를 통해서 눈으로 보는 것도 중요함\n",
    "3. 여러 클러스터링을 사용하여 최적의 군집개수(k)를 찾아보고 테스트 해보는 것을 추천함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 여러개의 클러스터링 개수를 List로 입력 받아 각각의 실루엣 계수를 면적으로 시각화한 함수 작성  \n",
    "def visualize_silhouette(cluster_model, cluster_lists, X_features): \n",
    "    \n",
    "    from sklearn.cluster import KMeans, MeanShift, estimate_bandwidth, DBSCAN\n",
    "    from sklearn.mixture import GaussianMixture\n",
    "    from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib.cm as cm\n",
    "    import math\n",
    "    \n",
    "    # 입력값으로 클러스터링 갯수들을 리스트로 받아서, 각 갯수별로 클러스터링을 적용하고 실루엣 개수를 구함\n",
    "    n_cols = len(cluster_lists)\n",
    "    \n",
    "    # plt.subplots()으로 리스트에 기재된 클러스터링 만큼의 sub figures를 가지는 axs 생성 \n",
    "    fig, axs = plt.subplots(figsize=(4*n_cols, 4), nrows=1, ncols=n_cols)\n",
    "    \n",
    "    # 리스트에 기재된 클러스터링 갯수들을 차례로 iteration 수행하면서 실루엣 개수 시각화\n",
    "    for ind, n_cluster in enumerate(cluster_lists):\n",
    "        \n",
    "        if cluster_model == 'kmeans' :\n",
    "            # KMeans 클러스터링 수행하고, 실루엣 스코어와 개별 데이터의 실루엣 값 계산. \n",
    "            clusterer = KMeans(n_clusters = n_cluster, max_iter=300, random_state=0)\n",
    "            \n",
    "        elif cluster_model == 'gmm' :\n",
    "            # GMM\n",
    "            clusterer = GaussianMixture(n_components=n_cluster, random_state=0)\n",
    "            \n",
    "        cluster_labels = clusterer.fit_predict(X_features)\n",
    "        sil_avg = silhouette_score(X_features, cluster_labels)\n",
    "        sil_values = silhouette_samples(X_features, cluster_labels)\n",
    "        \n",
    "        y_lower = 10\n",
    "        axs[ind].set_title('Number of Cluster : '+ str(n_cluster)+'\\n' \\\n",
    "                          'Silhouette Score :' + str(round(sil_avg,3)) )\n",
    "        axs[ind].set_xlabel(\"The silhouette coefficient values\")\n",
    "        axs[ind].set_ylabel(\"Cluster label\")\n",
    "        axs[ind].set_xlim([-0.1, 1])\n",
    "        axs[ind].set_ylim([0, len(X_features) + (n_cluster + 1) * 10])\n",
    "        axs[ind].set_yticks([])  # Clear the yaxis labels / ticks\n",
    "        axs[ind].set_xticks([0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "        \n",
    "        # 클러스터링 갯수별로 fill_betweenx( )형태의 막대 그래프 표현. \n",
    "        for i in range(n_cluster):\n",
    "            ith_cluster_sil_values = sil_values[cluster_labels==i]\n",
    "            ith_cluster_sil_values.sort()\n",
    "            \n",
    "            size_cluster_i = ith_cluster_sil_values.shape[0]\n",
    "            y_upper = y_lower + size_cluster_i\n",
    "            \n",
    "            color = cm.nipy_spectral(float(i) / n_cluster)\n",
    "            axs[ind].fill_betweenx(np.arange(y_lower, y_upper), 0, ith_cluster_sil_values, \\\n",
    "                                facecolor=color, edgecolor=color, alpha=0.7)\n",
    "            axs[ind].text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "            y_lower = y_upper + 10\n",
    "            \n",
    "        axs[ind].axvline(x=sil_avg, color=\"red\", linestyle=\"--\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (모델명, 군집개수 리스트, x_feature)\n",
    "visualize_silhouette('gmm', [2,3,4,5], X_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cluster별 Plot 생성\n",
    "* kmeans, GMM, DBSCAN 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 클러스터 결과를 담은 DataFrame과 사이킷런의 Cluster 객체등을 인자로 받아 클러스터링 결과를 시각화하는 함수  \n",
    "def visualize_cluster_plot(cluster_model, X_feature):\n",
    "    from sklearn.cluster import KMeans, DBSCAN\n",
    "    from sklearn.mixture import GaussianMixture\n",
    "    from sklearn.decomposition import PCA\n",
    "    \n",
    "    # PCA 변환 및 시각화를 위해서 2개로만 축소함\n",
    "    pca = PCA(n_components=2)\n",
    "    pca_df = pca.fit_transform(X_feature)\n",
    "    pca_df = pd.DataFrame(pca_df, columns=['pca1','pca2'])\n",
    "    \n",
    "    # 기본 2개 군집으로 해논 상태(k-menas, gmm 파라미터로 들어감)\n",
    "    n_clu = 3\n",
    "    \n",
    "    if cluster_model == 'kmeans' :\n",
    "        clusterer = KMeans(n_clusters=n_clu, init='k-means++', max_iter=500, random_state=0)\n",
    "        \n",
    "    elif cluster_model == 'gmm' :\n",
    "        clusterer = GaussianMixture(n_components=n_clu, random_state=0)\n",
    "        \n",
    "    elif cluster_model == 'dbscan' :\n",
    "        clusterer = DBSCAN(eps=0.5, min_samples=10, metric='euclidean')\n",
    "    \n",
    "    print(clusterer)\n",
    "    clusterer_label = clusterer.fit_predict(pca_df)\n",
    "    pca_df[cluster_model] = clusterer_label\n",
    "        \n",
    "    unique_labels = np.unique(pca_df[cluster_model].values)\n",
    "    markers=['o', 's', '^', 'x', '*']\n",
    "    isNoise=False\n",
    "    \n",
    "    # DBSCAN의 Noise 처리를 위해 필요한 함수\n",
    "    for label in unique_labels:\n",
    "        label_cluster = pca_df[pca_df[cluster_model]==label]\n",
    "        if label == -1:\n",
    "            cluster_legend = 'Noise'\n",
    "            isNoise=True\n",
    "        else :\n",
    "            cluster_legend = 'Cluster '+str(label)\n",
    "        \n",
    "        plt.scatter(x=label_cluster['pca1'], y=label_cluster['pca2'], s=70,\\\n",
    "                    edgecolor='k', marker=markers[label], label=cluster_legend)\n",
    "        \n",
    "    if isNoise:\n",
    "        legend_loc='upper center'\n",
    "    else: legend_loc='upper right'\n",
    "    \n",
    "    plt.legend(loc=legend_loc)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_cluster_plot('kmeans', iris.data)\n",
    "visualize_cluster_plot('gmm', iris.data)\n",
    "visualize_cluster_plot('dbscan', iris.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
